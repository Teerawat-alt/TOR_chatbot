import io, os
import streamlit as st
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from rag import ThaiTORRAG   
from htmltemplates import css, user_template, bot_template

load_dotenv()

# ---- model config----
EMBED_MODEL = "intfloat/multilingual-e5-base"
GEN_MODEL   = "Qwen/Qwen2.5-1.5B-Instruct"  # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö org/repo

# ---- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Ñ‡∏ä LLM (‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ----
@st.cache_resource(show_spinner=False)
def load_llm(model_id: str, hf_token: str | None):
    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏™‡πà VL ‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à
    low = model_id.lower()
    # if "vl" in low or "vision" in low:
    #     raise ValueError(f"‡∏£‡∏∏‡πà‡∏ô {model_id} ‡πÄ‡∏õ‡πá‡∏ô Vision-Language; ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô text-only")

    tok = AutoTokenizer.from_pretrained(model_id, token=hf_token, trust_remote_code=True)
    mdl = AutoModelForCausalLM.from_pretrained(
        model_id, device_map="auto", token=hf_token, trust_remote_code=True
    )
    gen = pipeline(
        "text-generation", model=mdl, tokenizer=tok,
        max_new_tokens=384, temperature=0.2, do_sample=False
    )
    return HuggingFacePipeline(pipeline=gen)

# ---- prepare token ----
HF_TOKEN = (os.getenv("HF_TOKEN")
            #or os.getenv("HUGGINGFACE_HUB_TOKEN")
            #or os.getenv("HUGGINGFACEHUB_API_TOKEN")
            )

# ---- ‡πÇ‡∏´‡∏•‡∏î/‡πÅ‡∏Ñ‡∏ä LLM ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ----
if "llm" not in st.session_state:
    st.session_state.llm = load_llm(GEN_MODEL, HF_TOKEN)

st.set_page_config(page_title="Thai TOR Chat (PDF)", page_icon="üìÑ")

st.header("‡πÅ‡∏ä‡∏ó‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ TOR ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢ üìÑüáπüá≠")
st.caption(f"Embedding: {EMBED_MODEL}  |  SLM: {GEN_MODEL}")

with st.sidebar:
    st.subheader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF (‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ)")
    pdf_files = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå", type=["pdf"], accept_multiple_files=True)

    if st.button("‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"):
        if not pdf_files:
            st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡πà‡∏≠‡∏ô")
        else:
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£..."):
                buffers = [io.BytesIO(f.read()) for f in pdf_files]
                rag = ThaiTORRAG(llm=st.session_state.llm, embed_model=EMBED_MODEL)
                rag.process_pdfs(buffers)
                st.session_state.rag = rag
                st.success("‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")

user_q = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö TOR:")
if user_q:
    if "rag" not in st.session_state or st.session_state.rag is None:
        st.error("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏Å‡∏î '‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£' ‡∏ó‡∏≤‡∏á‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")
    else:
        resp = st.session_state.rag.ask(user_q)
        for i, m in enumerate(resp.get("chat_history", [])):
            st.write(m.content)